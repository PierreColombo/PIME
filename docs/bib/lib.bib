@Article{cichocki2011generalized,
  author    = {Cichocki, Andrzej and Cruces, Sergio and Amari, Shun-ichi},
  title     = {Generalized alpha-beta divergences and their application to robust nonnegative matrix factorization},
  number    = {1},
  pages     = {134--170},
  volume    = {13},
  journal   = {Entropy},
  publisher = {Molecular Diversity Preservation International},
  year      = {2011},
}

@Article{agakov2004algorithm,
  author  = {Agakov, David Barber Felix},
  title   = {The im algorithm: a variational approach to information maximization},
  pages   = {201},
  volume  = {16},
  journal = {Advances in neural information processing systems},
  year    = {2004},
}

@Article{shannon2001mathematical,
  author    = {Shannon, Claude Elwood},
  title     = {A mathematical theory of communication},
  number    = {1},
  pages     = {3--55},
  volume    = {5},
  journal   = {ACM SIGMOBILE mobile computing and communications review},
  publisher = {ACM New York, NY, USA},
  year      = {2001},
}

@Book{cover1999elements,
  author    = {Cover, Thomas M},
  title     = {Elements of information theory},
  publisher = {John Wiley \& Sons},
  year      = {1999},
}

@Article{csiszar1967information,
  author  = {Csisz{\'a}r, Imre},
  title   = {Information-type measures of difference of probability distributions and indirect observation},
  pages   = {229--318},
  volume  = {2},
  journal = {studia scientiarum Mathematicarum Hungarica},
  year    = {1967},
}

@Article{peyre2019computational,
  author    = {Peyr{\'e}, Gabriel and Cuturi, Marco and others},
  title     = {Computational Optimal Transport: With Applications to Data Science},
  number    = {5-6},
  pages     = {355--607},
  volume    = {11},
  journal   = {Foundations and Trends{\textregistered} in Machine Learning},
  publisher = {Now Publishers, Inc.},
  year      = {2019},
}

@InProceedings{mcallester2020formal,
  author    = {McAllester, David and Stratos, Karl},
  booktitle = {International Conference on Artificial Intelligence and Statistics},
  title     = {Formal limitations on the measurement of mutual information},
  pages     = {875--884},
  year      = {2020},
}

@Article{gage1994new,
  author    = {Gage, Philip},
  title     = {A new algorithm for data compression},
  number    = {2},
  pages     = {23--38},
  volume    = {12},
  journal   = {C Users Journal},
  publisher = {McPherson, KS: R \& D Publications, c1987-1994.},
  year      = {1994},
}

@Article{saxe2019information,
  author    = {Saxe, Andrew M and Bansal, Yamini and Dapello, Joel and Advani, Madhu and Kolchinsky, Artemy and Tracey, Brendan D and Cox, David D},
  title     = {On the information bottleneck theory of deep learning},
  number    = {12},
  pages     = {124020},
  volume    = {2019},
  journal   = {Journal of Statistical Mechanics: Theory and Experiment},
  publisher = {IOP Publishing},
  year      = {2019},
}

@Article{clark2020electra,
  author  = {Clark, Kevin and Luong, Minh-Thang and Le, Quoc V and Manning, Christopher D},
  title   = {Electra: Pre-training text encoders as discriminators rather than generators},
  journal = {arXiv preprint arXiv:2003.10555},
  year    = {2020},
}

@Article{wiedemann2020deepcabac,
  author    = {Wiedemann, Simon and Kirchhoffer, Heiner and Matlage, Stefan and Haase, Paul and Marban, Arturo and Marin{\v{c}}, Talmaj and Neumann, David and Nguyen, Tung and Schwarz, Heiko and Wiegand, Thomas and others},
  title     = {Deepcabac: A universal compression algorithm for deep neural networks},
  number    = {4},
  pages     = {700--714},
  volume    = {14},
  journal   = {IEEE Journal of Selected Topics in Signal Processing},
  publisher = {IEEE},
  year      = {2020},
}

@Article{bregman1967relaxation,
  author    = {Bregman, Lev M},
  title     = {The relaxation method of finding the common point of convex sets and its application to the solution of problems in convex programming},
  number    = {3},
  pages     = {200--217},
  volume    = {7},
  journal   = {USSR computational mathematics and mathematical physics},
  publisher = {Elsevier},
  year      = {1967},
}

@Article{nce,
  author  = {Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
  title   = {Representation learning with contrastive predictive coding},
  journal = {arXiv preprint arXiv:1807.03748},
  year    = {2018},
}

@Misc{pichler2020estimation,
  author        = {Georg Pichler and Pablo Piantanida and Günther Koliander},
  title         = {On the Estimation of Information Measures of Continuous Distributions},
  eprint        = {2002.02851},
  archiveprefix = {arXiv},
  primaryclass  = {cs.IT},
  year          = {2020},
}

@Article{alemi2016deep,
  author  = {Alemi, Alexander A and Fischer, Ian and Dillon, Joshua V and Murphy, Kevin},
  title   = {Deep variational information bottleneck},
  journal = {arXiv preprint arXiv:1612.00410},
  year    = {2016},
}

@InProceedings{velickovic2019deep,
  author    = {Velickovic, Petar and Fedus, William and Hamilton, William L and Li{\`o}, Pietro and Bengio, Yoshua and Hjelm, R Devon},
  booktitle = {ICLR (Poster)},
  title     = {Deep Graph Infomax.},
  year      = {2019},
}

@Article{kong2019mutual,
  author  = {Kong, Lingpeng and d'Autume, Cyprien de Masson and Ling, Wang and Yu, Lei and Dai, Zihang and Yogatama, Dani},
  title   = {A mutual information maximization perspective of language representation learning},
  journal = {arXiv preprint arXiv:1910.08350},
  year    = {2019},
}

@Article{picot2022adversarial,
  author    = {Picot, Marine and Messina, Francisco and Boudiaf, Malik and Labeau, Fabrice and Ayed, Ismail Ben and Piantanida, Pablo},
  title     = {Adversarial Robustness via Fisher-Rao Regularization},
  journal   = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  publisher = {IEEE},
  year      = {2022},
}

@Article{kullback1951information,
  author    = {Kullback, Solomon and Leibler, Richard A},
  title     = {On Information and Sufficiency},
  doi       = {10.1214/AOMS/1177729694},
  number    = {1},
  pages     = {79--86},
  volume    = {22},
  file      = {:Kullback1951Information - On Information and Sufficiency.pdf:PDF},
  journal   = {The Annals of Mathematical Statistics},
  month     = mar,
  publisher = {JSTOR},
  year      = {1951},
}

@Article{mine,
  author  = {Belghazi, Mohamed Ishmael and Baratin, Aristide and Rajeswar, Sai and Ozair, Sherjil and Bengio, Yoshua and Courville, Aaron and Hjelm, R Devon},
  title   = {Mine: Mutual Information Neural Estimation},
  journal = {arXiv preprint arXiv:1801.04062},
  year    = {2018},
}

@Article{basseville2013divergence,
  author    = {Basseville, Mich{\'e}le},
  title     = {Divergence measures for statistical data processing—An annotated bibliography},
  number    = {4},
  pages     = {621--633},
  volume    = {93},
  journal   = {Signal Processing},
  publisher = {Elsevier},
  year      = {2013},
}

@Article{vincent2010stacked,
  author  = {Vincent, Pascal and Larochelle, Hugo and Lajoie, Isabelle and Bengio, Yoshua and Manzagol, Pierre-Antoine and Bottou, L{\'e}on},
  title   = {Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion.},
  number  = {12},
  volume  = {11},
  journal = {Journal of machine learning research},
  year    = {2010},
}

@Article{fujisawa2008robust,
  author    = {Fujisawa, Hironori and Eguchi, Shinto},
  title     = {Robust parameter estimation with a small bias against heavy contamination},
  number    = {9},
  pages     = {2053--2081},
  volume    = {99},
  journal   = {Journal of Multivariate Analysis},
  publisher = {Elsevier},
  year      = {2008},
}

@Article{cardoso1997infomax,
  author    = {Cardoso, J-F},
  title     = {Infomax and maximum likelihood for blind source separation},
  number    = {4},
  pages     = {112--114},
  volume    = {4},
  journal   = {IEEE Signal processing letters},
  publisher = {IEEE},
  year      = {1997},
}

@Article{blei2017variational,
  author    = {Blei, David M and Kucukelbir, Alp and McAuliffe, Jon D},
  title     = {Variational inference: A review for statisticians},
  number    = {518},
  pages     = {859--877},
  volume    = {112},
  journal   = {Journal of the American statistical Association},
  publisher = {Taylor \& Francis},
  year      = {2017},
}

@Article{crooks2017measures,
  author  = {Crooks, Gavin E},
  title   = {On measures of entropy and information},
  pages   = {v4},
  volume  = {9},
  journal = {Tech. Note},
  year    = {2017},
}

@Article{rao1987differential,
  author    = {Rao, C Radakrishna},
  title     = {Differential metrics in probability spaces},
  pages     = {217--240},
  volume    = {10},
  journal   = {Differential geometry in statistical inference},
  publisher = {IMS Hayward, Calif.},
  year      = {1987},
}


@Article{estimation,
  author    = {Paninski, Liam},
  title     = {Estimation of entropy and mutual information},
  number    = {6},
  pages     = {1191--1253},
  volume    = {15},
  journal   = {Neural computation},
  publisher = {MIT Press},
  year      = {2003},
}

@InProceedings{renyi1961measures,
  author       = {R{\'e}nyi, Alfr{\'e}d},
  booktitle    = {Proceedings of the fourth Berkeley symposium on mathematical statistics and probability},
  title        = {On measures of entropy and information},
  number       = {547-561},
  organization = {Berkeley, California, USA},
  volume       = {1},
  creationdate = {2022-09-21T13:19:55},
  file         = {:Renyi1961measures - On Measures of Entropy and Information.pdf:PDF},
  owner        = {georg},
  year         = {1961},
}

@InProceedings{cheng2020club,
  author    = {Cheng, Pengyu and Hao, Weituo and Dai, Shuyang and Liu, Jiachang and Gan, Zhe and Carin, Lawrence},
  booktitle = {International Conference on Machine Learning},
  title     = {{CLUB}: A contrastive log-ratio upper bound of mutual information},
  pages     = {1779--1788},
  year      = {2020},
  url       = {https://proceedings.mlr.press/v119/cheng20b.html}, 
}

@InProceedings{pmlr-v97-poole19a,
  author    = {Poole, Ben and Ozair, Sherjil and Van Den Oord, Aaron and Alemi, Alex and Tucker, George},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning},
  title     = {On Variational Bounds of Mutual Information},
  editor    = {Kamalika Chaudhuri and Ruslan Salakhutdinov},
  pages     = {5171--5180},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  url       = {http://proceedings.mlr.press/v97/poole19a.html},
  volume    = {97},
  abstract  = {Estimating and optimizing Mutual Information (MI) is core to many problems in machine learning, but bounding MI in high dimensions is challenging. To establish tractable and scalable objectives, recent work has turned to variational bounds parameterized by neural networks. However, the relationships and tradeoffs between these bounds remains unclear. In this work, we unify these recent developments in a single framework. We find that the existing variational lower bounds degrade when the MI is large, exhibiting either high bias or high variance. To address this problem, we introduce a continuum of lower bounds that encompasses previous bounds and flexibly trades off bias and variance. On high-dimensional, controlled problems, we empirically characterize the bias and variance of the bounds and their gradients and demonstrate the effectiveness of these new bounds for estimation and representation learning.},
  month     = {09--15 Jun},
  pdf       = {http://proceedings.mlr.press/v97/poole19a/poole19a.pdf},
  year      = {2019},
}

@Article{hellinger1909neue,
  author    = {Hellinger, Ernst},
  title     = {Neue Begr{\"u}ndung der Theorie quadratischer Formen von unendlichvielen Ver{\"a}nderlichen.},
  number    = {136},
  pages     = {210--271},
  volume    = {1909},
  journal   = {Journal f{\"u}r die reine und angewandte Mathematik},
  publisher = {De Gruyter},
  year      = {1909},
}

@Article{koliander2016entropy,
  author       = {Koliander, G{\"u}nther and Pichler, Georg and Riegler, Erwin and Hlawatsch, Franz},
  journal      = IEEE_J_IT,
  title        = {Entropy and Source Coding for Integer-Dimensional Singular Random Variables},
  year         = {2016},
  number       = {11},
  pages        = {6124--6154},
  volume       = {62},
  creationdate = {2017-05-14T00:00:00},
  doi          = {10.1109/TIT.2016.2604248},
  file         = {:Koliander2016Entropy - Entropy and Source Coding for Integer Dimensional Singular Random Variables.pdf:PDF},
  owner        = {georg},
  publisher    = {IEEE},
}

@InProceedings{pichler2022differential,
  author       = {Pichler, Georg and Colombo, Pierre Jean A. and Boudiaf, Malik and Koliander, G{\"u}nther and Piantanida, Pablo},
  booktitle    = {International Conference on Machine Learning},
  title        = {A Differential Entropy Estimator for Training Neural Networks},
  year         = {2022},
  month        = {17--23 Jul},
  pages        = {17691--17715},
  publisher    = {PMLR},
  series       = {Proceedings of Machine Learning Research},
  volume       = {162},
  abstract     = {Mutual Information (MI) has been widely used as a loss regularizer for training neural networks. This has been particularly effective when learn disentangled or compressed representations of high dimensional data. However, differential entropy (DE), another fundamental measure of information, has not found widespread use in neural network training. Although DE offers a potentially wider range of applications than MI, off-the-shelf DE estimators are either non differentiable, computationally intractable or fail to adapt to changes in the underlying distribution. These drawbacks prevent them from being used as regularizers in neural networks training. To address shortcomings in previously proposed estimators for DE, here we introduce KNIFE, a fully parameterized, differentiable kernel-based estimator of DE. The flexibility of our approach also allows us to construct KNIFE-based estimators for conditional (on either discrete or continuous variables) DE, as well as MI. We empirically validate our method on high-dimensional synthetic data and further apply it to guide the training of neural networks for real-world tasks. Our experiments on a large variety of tasks, including visual domain adaptation, textual fair classification, and textual fine-tuning demonstrate the effectiveness of KNIFE-based estimation. Code can be found at https://github.com/g-pichler/knife.},
  creationdate = {2022-09-28T14:05:57},
  pdf          = {https://proceedings.mlr.press/v162/pichler22a/pichler22a.pdf},
  url          = {https://proceedings.mlr.press/v162/pichler22a.html},
}

@Article{nguyen2010estimating,
  author       = {Nguyen, XuanLong and Wainwright, Martin J and Jordan, Michael I},
  journal      = {IEEE Transactions on Information Theory},
  title        = {Estimating divergence functionals and the likelihood ratio by convex risk minimization},
  year         = {2010},
  number       = {11},
  pages        = {5847--5861},
  volume       = {56},
  creationdate = {2019-07-04T00:00:00},
  publisher    = {IEEE},
}

@InProceedings{belghazi2018mutual,
  author       = {Belghazi, Mohamed Ishmael and Baratin, Aristide and Rajeshwar, Sai and Ozair, Sherjil and Bengio, Yoshua and Courville, Aaron and Hjelm, Devon},
  booktitle    = {International Conference on Machine Learning},
  title        = {Mutual Information Neural Estimation},
  year         = {2018},
  address      = {Stockholmsm{\"{a}}ssan, Stockholm Sweden},
  editor       = {Dy, Jennifer and Krause, Andreas},
  month        = jul,
  pages        = {531--540},
  publisher    = {PMLR},
  series       = {Proceedings of Machine Learning Research},
  volume       = {80},
  abstract     = {We argue that the estimation of mutual information between high dimensional continuous random variables can be achieved by gradient descent over neural networks. We present a Mutual Information Neural Estimator (MINE) that is linearly scalable in dimensionality as well as in sample size, trainable through back-prop, and strongly consistent. We present a handful of applications on which MINE can be used to minimize or maximize mutual information. We apply MINE to improve adversarially trained generative models. We also use MINE to implement the Information Bottleneck, applying it to supervised classification; our results demonstrate substantial improvement in flexibility and performance in these settings.},
  creationdate = {2019-12-17T00:00:00},  
  url          = {http://proceedings.mlr.press/v80/belghazi18a.html},
}

@inproceedings{alemi2017deep,
title={Deep Variational Information Bottleneck},
author={Alexander A. Alemi and Ian Fischer and Joshua V. Dillon and Kevin Murphy},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=HyxQzBceg}
}

@inproceedings{colombo2021code,
    title = "Code-switched inspired losses for spoken dialog representations",
    author = "Colombo, Pierre  and
      Chapuis, Emile  and
      Labeau, Matthieu  and
      Clavel, Chlo{\'e}",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.656",
    doi = "10.18653/v1/2021.emnlp-main.656",
    pages = "8320--8337",
    abstract = "Spoken dialogue systems need to be able to handle both multiple languages and multilinguality inside a conversation (\textit{e.g} in case of code-switching). In this work, we introduce new pretraining losses tailored to learn generic multilingual spoken dialogue representations. The goal of these losses is to expose the model to code-switched language. In order to scale up training, we automatically build a pretraining corpus composed of multilingual conversations in five different languages (French, Italian, English, German and Spanish) from OpenSubtitles, a huge multilingual corpus composed of 24.3G tokens. We test the generic representations on MIAM, a new benchmark composed of five dialogue act corpora on the same aforementioned languages as well as on two novel multilingual tasks (\textit{i.e} multilingual mask utterance retrieval and multilingual inconsistency identification). Our experiments show that our new losses achieve a better performance in both monolingual and multilingual settings.",
}

@InProceedings{chapuis2020hierarchical,
  author       = {Chapuis, Emile and Colombo, Pierre and Manica, Matteo and Labeau, Matthieu and Clavel, Chlo{\'e}},
  booktitle    = {Findings of the Association for Computational Linguistics: EMNLP 2020},
  title        = {Hierarchical Pre-training for Sequence Labelling in Spoken Dialog},
  year         = {2020},
  address      = {Online},
  month        = nov,
  pages        = {2636--2648},
  publisher    = {Association for Computational Linguistics},
  abstract     = {Sequence labelling tasks like Dialog Act and Emotion/Sentiment identification are a key component of spoken dialog systems. In this work, we propose a new approach to learn generic representations adapted to spoken dialog, which we evaluate on a new benchmark we call Sequence labellIng evaLuatIon benChmark fOr spoken laNguagE benchmark (SILICONE). SILICONE is model-agnostic and contains 10 different datasets of various sizes. We obtain our representations with a hierarchical encoder based on transformer architectures, for which we extend two well-known pre-training objectives. Pre-training is performed on OpenSubtitles: a large corpus of spoken dialog containing over 2.3 billion of tokens. We demonstrate how hierarchical encoders achieve competitive results with consistently fewer parameters compared to state-of-the-art models and we show their importance for both pre-training and fine-tuning.},
  creationdate = {2022-09-29T14:05:13},
  doi          = {10.18653/v1/2020.findings-emnlp.239},
  url          = {https://aclanthology.org/2020.findings-emnlp.239},
}

@InProceedings{kong2020mutual,
  author       = {Lingpeng Kong and Cyprien de Masson d'Autume and Lei Yu and Wang Ling and Zihang Dai and Dani Yogatama},
  booktitle    = {International Conference on Learning Representations},
  title        = {A Mutual Information Maximization Perspective of Language Representation Learning},
  year         = {2020},
  creationdate = {2022-09-29T14:07:51},
  url          = {https://openreview.net/forum?id=Syx79eBKwr},
}


@Article{mcallester2018formal,
  author       = {David McAllester and Karl Stratos},
  journal      = {arXiv preprint},
  title        = {Formal Limitations on the Measurement of Mutual Information},
  year         = {2018},
  abstract     = {Motivated by applications to unsupervised learning, we consider the problem of measuring mutual information. Recent analysis has shown that naive kNN estimators of mutual information have serious statistical limitations motivating more refined methods. In this paper we prove that serious statistical limitations are inherent to any measurement method. More specifically, we show that any distribution-free high-confidence lower bound on mutual information cannot be larger than $O(\ln N)$ where $N$ is the size of the data sample. We also analyze the Donsker-Varadhan lower bound on KL divergence in particular and show that, when simple statistical considerations are taken into account, this bound can never produce a high-confidence value larger than $\ln N$. While large high-confidence lower bounds are impossible, in practice one can use estimators without formal guarantees. We suggest expressing mutual information as a difference of entropies and using cross-entropy as an entropy estimator. We observe that, although cross-entropy is only an upper bound on entropy, cross-entropy estimates converge to the true cross-entropy at the rate of $1/\sqrt{N}$.},
  creationdate = {2019-09-22T00:00:00},
  date         = {2018-11-10},
  eprint       = {http://arxiv.org/abs/1811.04251v4},
  eprintclass  = {cs.IT},
  eprinttype   = {arXiv},
}

@Article{Csiszar2003Information,
  author       = {Csiszar, I. and Matus, F.},
  journal      = {IEEE Transactions on Information Theory},
  title        = {Information projections revisited},
  year         = {2003},
  number       = {6},
  pages        = {1474-1490},
  volume       = {49},
  creationdate = {2022-10-13T12:47:35},
  doi          = {10.1109/TIT.2003.810633},
  owner        = {georg},
}

@Article{Colombo2022InfoLM,
  author       = {Colombo, Pierre Jean A. and Clavel, Chloé and Piantanida, Pablo},
  journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
  title        = {InfoLM: A New Metric to Evaluate Summarization &amp; Data2Text Generation},
  year         = {2022},
  month        = {Jun.},
  number       = {10},
  pages        = {10554-10562},
  volume       = {36},
  abstractnote = {Assessing the quality of natural language generation (NLG) systems through human annotation is very expensive. Additionally, human annotation campaigns are time-consuming and include non-reusable human labour. In practice, researchers rely on automatic metrics as a proxy of quality. In the last decade, many string-based metrics (e.g., BLEU or ROUGE) have been introduced. However, such metrics usually rely on exact matches and thus, do not robustly handle synonyms. In this paper, we introduce InfoLM a family of untrained metrics that can be viewed as a string-based metric that addresses the aforementioned flaws thanks to a pre-trained masked language model. This family of metrics also makes use of information measures allowing the possibility to adapt InfoLM to different evaluation criteria. Using direct assessment, we demonstrate that InfoLM achieves statistically significant improvement and two figure correlation gains in many configurations compared to existing metrics on both summarization and data2text generation tasks.},
  creationdate = {2022-10-13T12:57:33},
  doi          = {10.1609/aaai.v36i10.21299},
  owner        = {georg},
  url          = {https://ojs.aaai.org/index.php/AAAI/article/view/21299},
}

@Book{Renyi2007Probability,
  author       = {R{\'e}nyi, Alfr{\'e}d},
  publisher    = {Courier Corporation},
  title        = {Probability theory},
  year         = {2007},
  creationdate = {2022-10-13T13:19:06},
  owner        = {georg},
}

@article{basu1998robust,
  title={Robust and efficient estimation by minimising a density power divergence},
  author={Basu, Ayanendranath and Harris, Ian R and Hjort, Nils L and Jones, MC},
  journal={Biometrika},
  volume={85},
  number={3},
  pages={549--559},
  year={1998},
  publisher={Oxford University Press}
}

@Article{VanErven2014Renyi,
  author       = {Van Erven, Tim and Harremos, Peter},
  journal      = {IEEE Transactions on Information Theory},
  title        = {R{\'e}nyi divergence and Kullback-Leibler divergence},
  year         = {2014},
  number       = {7},
  pages        = {3797--3820},
  volume       = {60},
  creationdate = {2019-07-04T00:00:00},
  file         = {:VanErven2014Renyi - Renyi Divergence and Kullback Leibler Divergence.pdf:PDF},
  owner        = {georg},
  publisher    = {IEEE},
}

@InProceedings{Colombo2022Learning,
  author       = {Colombo, Pierre and Staerman, Guillaume and Noiry, Nathan and Piantanida, Pablo},
  booktitle    = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  title        = {Learning Disentangled Textual Representations via Statistical Measures of Similarity},
  year         = {2022},
  address      = {Dublin, Ireland},
  month        = may,
  pages        = {2614--2630},
  publisher    = {Association for Computational Linguistics},
  abstract     = {When working with textual data, a natural application of disentangled representations is the fair classification where the goal is to make predictions without being biased (or influenced) by sensible attributes that may be present in the data (e.g., age, gender or race). Dominant approaches to disentangle a sensitive attribute from textual representations rely on learning simultaneously a penalization term that involves either an adversary loss (e.g., a discriminator) or an information measure (e.g., mutual information). However, these methods require the training of a deep neural network with several parameter updates for each update of the representation model. As a matter of fact, the resulting nested optimization loop is both times consuming, adding complexity to the optimization dynamic, and requires a fine hyperparameter selection (e.g., learning rates, architecture). In this work, we introduce a family of regularizers for learning disentangled representations that do not require training. These regularizers are based on statistical measures of similarity between the conditional probability distributions with respect to the sensible attributes. Our novel regularizers do not require additional training, are faster and do not involve additional tuning while achieving better results both when combined with pretrained and randomly initialized text encoders.},
  creationdate = {2022-10-13T13:39:44},
  doi          = {10.18653/v1/2022.acl-long.187},
  owner        = {georg},
  url          = {https://aclanthology.org/2022.acl-long.187},
}

@Article{Costa2015Fisher,
  author       = {Sueli I.R. Costa and Sandra A. Santos and João E. Strapasson},
  journal      = {Discrete Applied Mathematics},
  title        = {Fisher information distance: A geometrical reading},
  year         = {2015},
  issn         = {0166-218X},
  note         = {Distance Geometry and Applications},
  pages        = {59-69},
  volume       = {197},
  abstract     = {This paper presents a geometrical approach to the Fisher distance, which is a measure of dissimilarity between two probability distribution functions. The Fisher distance, as well as other divergence measures, is also used in many applications to establish a proper data average. The main purpose is to widen the range of possible interpretations and relations of the Fisher distance and its associated geometry for the prospective applications. It focuses on statistical models of the normal probability distribution functions and takes advantage of the connection with the classical hyperbolic geometry to derive closed forms for the Fisher distance in several cases. Connections with the well-known Kullback–Leibler divergence measure are also devised.},
  creationdate = {2022-10-13T13:42:40},
  doi          = {https://doi.org/10.1016/j.dam.2014.10.004},
  keywords     = {Fisher distance, Information geometry, Normal probability distribution functions, Kullback–Leibler divergence, Hyperbolic geometry},
  owner        = {georg},
  url          = {https://www.sciencedirect.com/science/article/pii/S0166218X14004211},
}

@Comment{jabref-meta: databaseType:bibtex;}
